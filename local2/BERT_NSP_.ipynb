{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_NSP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC2Ue2SRZrPr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "f2e51555-e054-404f-9488-1b2bf555e77c"
      },
      "source": [
        "!pip install transformers\n",
        "# !pip install seqeval"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.38)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.38)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (0.0.12)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwNK_oEYdx8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertForNextSentencePrediction, BertTokenizer, AdamW\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "import string\n",
        "# from seqeval.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ua0Zk67axto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "        self.data = None\n",
        "        with open(path, 'r') as f:\n",
        "            self.data = f.readlines()\n",
        "            if len(self.data) > 10000:\n",
        "              self.data = self.data[:15000]\n",
        "            self.data = [x.translate(str.maketrans('', '', string.punctuation)) for x in self.data]\n",
        "            self.data = [x for x in self.data if len(x.split())>=2]\n",
        "            self.lengths = [len(y.split()) for y in self.data]\n",
        "            # print(self.lengths)\n",
        "            self.substr_len = 2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.lengths[index]\n",
        "        try:\n",
        "            neg_x = self.data[index + 1]\n",
        "            neg_y = self.lengths[index + 1]\n",
        "        except:\n",
        "            neg_x = self.data[index - 1]\n",
        "            neg_y = self.lengths[index - 1]\n",
        "        try:\n",
        "          rand_idx_pos = np.random.randint(0, y - self.substr_len - 1)\n",
        "        except:\n",
        "          rand_idx_pos = np.random.randint(0, y - 1)\n",
        "        try:\n",
        "          rand_idx_neg = np.random.randint(0, neg_y - self.substr_len - 1)\n",
        "        except:\n",
        "          rand_idx_neg = np.random.randint(0, neg_y - 1)\n",
        "        \n",
        "\n",
        "\n",
        "        return x, \\\n",
        "               \" \".join(x.split()[rand_idx_pos:rand_idx_pos + self.substr_len]), \\\n",
        "               \" \".join(neg_x.split()[rand_idx_neg:rand_idx_neg + self.substr_len])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DDVWVrra3li",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/DirectedStudy/BERT_NSP/wikitext-2'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvvk9r5od5iq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = CustomDataset(os.path.join(path, \"train_.txt\"))\n",
        "valid_dataset = CustomDataset(os.path.join(path, \"valid_.txt\"))\n",
        "test_dataset = CustomDataset(os.path.join(path, \"test_.txt\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9X6cP_Md8Kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBl7AyPBeFT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 4\n",
        "num_workers = 8 if cuda else multiprocessing.cpu_count()\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
        "valid_dataloader = DataLoader(valid_dataset, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size, num_workers=num_workers)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ0F_aDNeILN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model.to(device)\n",
        "lr = 3e-5\n",
        "# optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "num_epochs = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14gxTK7feJyE",
        "colab_type": "code",
        "outputId": "84c99f40-7d9b-460a-939d-58b2a6b8a682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"len(trainloader): \", len(train_dataloader), \"batch_size: \", batch_size,\n",
        "      \"len(train_dataloader)//batch_size - 1: \", len(train_dataloader) // batch_size - 1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(trainloader):  3561 batch_size:  4 len(train_dataloader)//batch_size - 1:  889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDAUP3jYeL6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader, valid_loader, test_loader, optimizer, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"Epoch {}:\".format(epoch))\n",
        "        model.train()\n",
        "        num_correct = 0\n",
        "        running_loss = 0.\n",
        "        num_total = 0\n",
        "        label_ids = list()\n",
        "        preds_list = list()\n",
        "        for batch_num, d in enumerate(train_loader):\n",
        "            pos, pos_sub, neg_sub = d[0], d[1], d[2]\n",
        "            # print(pos)\n",
        "            # print(pos_sub)\n",
        "            # print(neg_sub)\n",
        "            labels = torch.cuda.LongTensor(np.zeros((len(pos) * 2), float))\n",
        "            labels[len(pos):] = 1\n",
        "            # print(labels)\n",
        "            labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pairofstrings = list(zip(pos, pos_sub))\n",
        "            pairofstrings.extend(list(zip(pos, neg_sub)))\n",
        "            del pos\n",
        "            del pos_sub\n",
        "            del neg_sub\n",
        "            encoded_batch = tokenizer.batch_encode_plus(pairofstrings, add_special_tokens=True, return_tensors='pt',\n",
        "                                                        return_special_tokens_masks=True, max_length=512,\n",
        "                                                        pad_to_max_length=True)\n",
        "            attention_mask = (encoded_batch['attention_mask'] - encoded_batch['special_tokens_mask']).to(device)\n",
        "            input_ids, token_type_ids = encoded_batch['input_ids'].to(device), encoded_batch['token_type_ids'].to(\n",
        "                device)\n",
        "            loss, logits = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,\n",
        "                                 next_sentence_label=labels)\n",
        "            # print(logits)\n",
        "            predicted = torch.max(logits, 1)[1]\n",
        "            # predicted = torch.max(logits, 1)[1]\n",
        "            num_total += labels.size(0)\n",
        "            # print(\"predicted:\", predicted)\n",
        "            # print(\"labels:\", labels)\n",
        "            num_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            optimizer.step()\n",
        "            # print(input_ids.shape)\n",
        "            # print(labels.shape)\n",
        "            batch_size, max_seq_len = input_ids.shape\n",
        "            for b in range(batch_size):\n",
        "                label_ids_temp = []\n",
        "                preds_list_temp = []\n",
        "                label_ids_temp.append(labels[b].item())\n",
        "                preds_list_temp.append(predicted[b].item())\n",
        "                label_ids.extend(label_ids_temp.copy())\n",
        "                preds_list.extend(preds_list_temp.copy())\n",
        "\n",
        "            del labels\n",
        "            del loss\n",
        "\n",
        "            if batch_num % 100 == 0 or batch_num == len(train_dataloader) // batch_size - 1:\n",
        "                print(\"acc : \", (num_correct) / num_total, \"batch_num:\", batch_num)\n",
        "                torch.save(model.state_dict(), 'model.npy')\n",
        "                torch.save(optimizer.state_dict(), 'optimizer.npy')\n",
        "\n",
        "        print('Train Accuracy: {}'.format(num_correct / num_total),\n",
        "              'Average Train Loss: {}'.format(running_loss / len(train_loader)))\n",
        "        # print(type(label_ids), type(preds_list))\n",
        "        # # label_ids, preds_list = label_ids.flatten(), preds_list.flatten()\n",
        "        # print(label_ids, preds_list)\n",
        "        # print(len(label_ids), len(preds_list))\n",
        "        print(\"Precision: \" + str(precision_score(label_ids, preds_list)))\n",
        "        print(\"Recall: \" + str(recall_score(label_ids, preds_list)))\n",
        "        print(\"F1: \" + str(f1_score(label_ids, preds_list)))\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            ep_num = epoch + 1\n",
        "            torch.save(model.state_dict(), 'model' + str(ep_num) + '.npy')\n",
        "            torch.save(optimizer.state_dict(), 'optimizer' + str(ep_num) + '.npy')\n",
        "\n",
        "        model.eval()\n",
        "        num_correct = 0\n",
        "        running_loss = 0.\n",
        "        num_total = 0\n",
        "        with torch.no_grad():\n",
        "            label_ids = list()\n",
        "            preds_list = list()\n",
        "            for batch_num, d in enumerate(valid_loader):\n",
        "                pos, pos_sub, neg_sub = d[0], d[1], d[2]\n",
        "                labels = torch.cuda.LongTensor(np.zeros((len(pos) * 2), float))\n",
        "                labels[len(pos):] = 1\n",
        "                labels.to(device)\n",
        "                pairofstrings = list(zip(pos, pos_sub))\n",
        "                pairofstrings.extend(list(zip(pos, neg_sub)))\n",
        "                del pos\n",
        "                del pos_sub\n",
        "                del neg_sub\n",
        "                encoded_batch = tokenizer.batch_encode_plus(pairofstrings, add_special_tokens=True, return_tensors='pt',\n",
        "                                                            max_length=512, return_special_tokens_masks=True,\n",
        "                                                            pad_to_max_length=True)\n",
        "                attention_mask = (encoded_batch['attention_mask'] - encoded_batch['special_tokens_mask']).to(device)\n",
        "                input_ids, token_type_ids = encoded_batch['input_ids'].to(device), encoded_batch['token_type_ids'].to(\n",
        "                    device)\n",
        "                loss, logits = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,\n",
        "                                     next_sentence_label=labels)\n",
        "                predicted = torch.max(logits, 1)[1]\n",
        "                # print(\"labels:\")\n",
        "                num_total += labels.size(0)\n",
        "                num_correct += (predicted == labels).sum().item()\n",
        "                running_loss += loss.item()\n",
        "                print(\"predicted, labels:\", predicted.cpu().detach().numpy(), labels.cpu().detach().numpy())\n",
        "                batch_size, max_seq_len = input_ids.shape\n",
        "                for b in range(batch_size):\n",
        "                    label_ids_temp = []\n",
        "                    preds_list_temp = []\n",
        "                    label_ids_temp.append(labels[b].item())\n",
        "                    preds_list_temp.append(predicted[b].item())\n",
        "                    label_ids.extend(label_ids_temp.copy())\n",
        "                    preds_list.extend(preds_list_temp.copy())\n",
        "\n",
        "                del labels\n",
        "                del loss\n",
        "\n",
        "        print('Validation Accuracy: {}'.format(num_correct / num_total),\n",
        "              'Average Validation Loss: {}'.format(running_loss / len(valid_loader)))\n",
        "        print(\"Precision: \" + str(precision_score(label_ids, preds_list)))\n",
        "        print(\"Recall: \" + str(recall_score(label_ids, preds_list)))\n",
        "        print(\"F1: \" + str(f1_score(label_ids, preds_list)))\n",
        "        num_correct = 0\n",
        "        running_loss = 0.\n",
        "        num_total = 0\n",
        "        with torch.no_grad():\n",
        "            label_ids = list()\n",
        "            preds_list = list()\n",
        "            for batch_num, d in enumerate(test_loader):\n",
        "                pos, pos_sub, neg_sub = d[0], d[1], d[2]\n",
        "                labels = torch.cuda.LongTensor(np.zeros((len(pos) * 2), float))\n",
        "                labels[len(pos):] = 1\n",
        "                labels.to(device)\n",
        "                pairofstrings = list(zip(pos, pos_sub))\n",
        "                pairofstrings.extend(list(zip(pos, neg_sub)))\n",
        "                del pos\n",
        "                del pos_sub\n",
        "                del neg_sub\n",
        "                encoded_batch = tokenizer.batch_encode_plus(pairofstrings, add_special_tokens=True, return_tensors='pt',\n",
        "                                                            max_length=512, return_special_tokens_masks=True,\n",
        "                                                            pad_to_max_length=True)\n",
        "                attention_mask = (encoded_batch['attention_mask'] - encoded_batch['special_tokens_mask']).to(device)\n",
        "                input_ids, token_type_ids = encoded_batch['input_ids'].to(device), encoded_batch['token_type_ids'].to(\n",
        "                    device)\n",
        "                loss, logits = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,\n",
        "                                     next_sentence_label=labels)\n",
        "                predicted = torch.max(logits, 1)[1]\n",
        "                # print(\"labels:\")\n",
        "                num_total += labels.size(0)\n",
        "                num_correct += (predicted == labels).sum().item()\n",
        "                running_loss += loss.item()\n",
        "                batch_size, max_seq_len = input_ids.shape\n",
        "                for b in range(batch_size):\n",
        "                    label_ids_temp = []\n",
        "                    preds_list_temp = []\n",
        "                    label_ids_temp.append(labels[b].item())\n",
        "                    preds_list_temp.append(predicted[b].item())\n",
        "                    label_ids.extend(label_ids_temp.copy())\n",
        "                    preds_list.extend(preds_list_temp.copy())\n",
        "\n",
        "                del labels\n",
        "                del loss\n",
        "        print('Test Accuracy: {}'.format(num_correct / num_total),\n",
        "              'Average Test Loss: {}'.format(running_loss / len(valid_loader)))\n",
        "        print(\"Precision: \" + str(precision_score(label_ids, preds_list)))\n",
        "        print(\"Recall: \" + str(recall_score(label_ids, preds_list)))\n",
        "        print(\"F1: \" + str(f1_score(label_ids, preds_list)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40m_tSQxipyw",
        "colab_type": "code",
        "outputId": "489d050c-2756-4938-a864-5d04227ffe2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(device)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY-rFpPReO9I",
        "colab_type": "code",
        "outputId": "bb5fb9cc-3c46-48c1-fad5-3a14b26e556c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train(model, train_dataloader, valid_dataloader, test_dataloader, optimizer, num_epochs)\n",
        "\n",
        "torch.save(model.state_dict(), 'model_final.npy')\n",
        "torch.save(optimizer.state_dict(), 'optimizer_final.npy')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0:\n",
            "acc :  0.625 batch_num: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_RqKxiuc5oT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFq5VSG2lDej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}