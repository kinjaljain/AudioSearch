# -*- coding: utf-8 -*-
"""QA_BERT_BASELINE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D8heHmgZpBt94Sbn9z07v5cwk552KEIK
"""


# 1. MLM
# 2. NSP on BERT
# 2.1 Use every 5th sentence and its next sentence
# 3. NSP for the actual task
#   3.1 - train on 2018 + 2019 data
#   3.2 - use sentences predicted from the rake + similarity model to finetune on this task


# Other approaches:
# 1. SciBERT, Roberta, ALBERT -> use them as is and also compare them with MLM fine-tuned versions of each of these models


import torch
from torch.nn.functional import softmax
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from transformers import BertForNextSentencePrediction, BertTokenizer, AdamW
import json
import multiprocessing
import numpy as np


class CiteDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, item):
        # print(type(self.data[item]))
        x = self.data[item]
        return " " + x[0][:511].lower(), " " + x[1][:511].lower(), int(not (x[2]))


class CiteDataset2018(Dataset):  # for prepared_data.json
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, item):
        # print(type(self.data[item]))
        x = self.data[item]
        return x[0].lower(), x[1].lower(), int(not (x[2]))


# 2018 data
with open('prepared_data_nsp_actual_1st_qtr.json', 'r') as f:
    prepared_data_2018 = json.load(f)

# with open('BERT_BASELINE/prepared_data_with_context_title_section_without_extra_cites2.json', 'r') as f:
#     prepared_data_2019 = json.load(f)

prepared_data = prepared_data_2018

cuda = torch.cuda.is_available()
device = torch.device("cuda" if cuda else "cpu")

# load pretrained model and a pretrained tokenizer
model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

model.to(device)

train, valid = train_test_split(prepared_data, test_size=0.2)
train_dataset = CiteDataset(train)
valid_dataset = CiteDataset(valid)
batch_size = 1

num_workers = 8 if cuda else multiprocessing.cpu_count()
train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, num_workers=num_workers)
valid_dataloader = DataLoader(valid_dataset, shuffle=False, batch_size=batch_size, num_workers=num_workers)

print("len(trainloader)", len(train_dataloader), "batch_size", batch_size, "len(train_dataloader)//batch_size - 1",
      len(train_dataloader) // batch_size - 1)

true_batch_size = 16

def train(model, train_loader, valid_loader, optimizer, num_epochs):
    for epoch in range(num_epochs):
        print("Epoch {}:".format(epoch))
        model.train()
        num_correct = 0
        running_loss = 0.
        num_total = 0
        for batch_num, d in enumerate(train_loader):
            refs, cites, labels = d[0], d[1], d[2].to(device)
            if batch_num % true_batch_size == 0:
                optimizer.zero_grad()
            # print("refs:", refs)
            # print("len of refs:", len(refs))
            # print("cites:", cites)
            # print("len of cites:", len(cites))
            pairofstrings = list(zip(refs, cites))
            encoded_batch = tokenizer.batch_encode_plus(pairofstrings, add_special_tokens=True, return_tensors='pt',
                                                        return_special_tokens_masks=True, max_length=512,
                                                        pad_to_max_length=True)
            attention_mask = (encoded_batch['attention_mask'] - encoded_batch['special_tokens_mask']).to(device)
            input_ids, token_type_ids = encoded_batch['input_ids'].to(device), encoded_batch['token_type_ids'].to(
                device)
            loss, logits = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,
                                 next_sentence_label=labels)
            # logits = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]
            # print("outputs:")
            # print(logits)
            predicted = torch.max(logits, 1)[1]
            num_total += labels.size(0)
            # print("predicted:", predicted)
            # print("labels:", labels)
            num_correct += (predicted == labels).sum().item()

            # loss = criterion(outputs, labels)
            loss.backward()
            running_loss += loss.item()
            # todo check if the batch_num == len(train_dataloader) - 1 constraint works
            if batch_num % true_batch_size == 0 or batch_num == len(train_dataloader) // true_batch_size - 1:
                optimizer.step()
            if batch_num % 100 == 0 or batch_num == len(train_dataloader) // batch_size - 1:
                print("acc : ", (num_correct) / num_total, "batch_num:", batch_num)
            if batch_num % 2000 == 0:
                torch.save(model.state_dict(), 'model.npy')
                torch.save(optimizer.state_dict(), 'optimizer.npy')

        print('Train Accuracy: {}'.format(num_correct / num_total),
              'Average Loss: {}'.format(running_loss / len(train_loader)))

        if epoch % 1 == 0:
            ep_num = epoch + 1
            torch.save(model.state_dict(), 'model' + str(ep_num) + '.npy')
            torch.save(optimizer.state_dict(), 'optimizer' + str(ep_num) + '.npy')

        model.eval()
        num_correct = 0
        running_loss = 0.
        num_total = 0
        with torch.no_grad():
            for batch_num, d in enumerate(valid_loader):
                refs, cites, labels = d[0], d[1], d[2].to(device)
                pairofstrings = list(zip(refs, cites))
                encoded_batch = tokenizer.batch_encode_plus(pairofstrings, add_special_tokens=True, return_tensors='pt',
                                                            max_length=512, return_special_tokens_masks=True,
                                                            pad_to_max_length=True)
                attention_mask = (encoded_batch['attention_mask'] - encoded_batch['special_tokens_mask']).to(device)
                input_ids, token_type_ids = encoded_batch['input_ids'].to(device), encoded_batch['token_type_ids'].to(
                    device)
                loss, logits = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,
                                     next_sentence_label=labels)
                predicted = torch.max(logits, 1)[1]
                # print("labels:")
                num_total += labels.size(0)
                num_correct += (predicted == labels).sum().item()
                running_loss += loss.item()
                print("predicted, labels:", predicted.cpu().detach().numpy(), labels.cpu().detach().numpy())

        print('Validation Accuracy: {}'.format(num_correct / num_total),
              'Average Loss: {}'.format(running_loss / len(valid_loader)))


model.to(device)
lr = 3e-5
# optimizer = optim.SGD(model.parameters(), lr=lr)
optimizer = AdamW(model.parameters(), lr=lr)
num_epochs = 25

train(model, train_dataloader, valid_dataloader, optimizer, num_epochs)

torch.save(model.state_dict(), 'model.npy')
torch.save(optimizer.state_dict(), 'optimizer.npy')

#
# num_epochs = 15
# train(model, train_dataloader, valid_dataloader, optimizer, criterion, num_epochs)
#
# torch.save(model.state_dict(), 'model.npy')
# torch.save(optimizer.state_dict(), 'optimizer.npy')
