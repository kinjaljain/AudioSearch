# -*- coding utf-8 -*-
"""BERT_NSP_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lgNfF609uYaETTO4svyJP7stchS5zAZT
"""

# pip install transformers

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertForNextSentencePrediction, BertTokenizer, AdamW
import multiprocessing
import numpy as np
import os
import string
from sklearn.metrics import f1_score, precision_score, recall_score
from collections import defaultdict
from torch.utils import data as data_utils
# from hyperparameters import hparams, hparams_debug_string

os.environ["CUDA_VISIBLE_DEVICES"]='1'
vox_dir = '/home/kinjalj/libri/vox'

def get_fnames(fnames_file):
    filenames_array = []
    f = open(fnames_file)
    for line in f:
        line = line.split('\n')[0]
        filenames_array.append(line)
    return filenames_array


def get_featmetainfo(desc_file, feat_name):
    f = open(desc_file)
    for line in f:
        line = line.split('\n')[0]
        feat = line.split('|')[0]
        if feat_name == feat:
            feat_length, feat_type = line.split('|')[1], line.split('|')[2]
            return feat_length, feat_type

class FloatDataSource(Dataset):
    """
    Syntax
    dataset = FloatDataSource(fnames.txt.train, etc/falcon_feats.desc, feat_name, feats_dir)
    """

    def __init__(self, fnames_file, desc_file, feat_name, feats_dir, feats_dict=None):
        self.fnames_file = fnames_file
        self.feat_name = feat_name
        self.desc_file = desc_file
        self.filenames_array = get_fnames(self.fnames_file)
        self.feat_length, self.feat_type = get_featmetainfo(self.desc_file, feat_name)
        self.feats_dir = feats_dir
        self.feats_dict = defaultdict(lambda: len(self.feats_dict)) if feats_dict is None else feats_dict

    def __getitem__(self, idx):

        fname = self.filenames_array[idx].split('.')[0]
        if self.feat_name == 'f0':
            fname = self.feats_dir + '/' + fname.strip() + '.feats'
            feats_array = np.loadtxt(fname)

        else:
            fname = self.feats_dir + '/' + fname.strip() + '.feats.npy'
            feats_array = np.load(fname)
        return feats_array

    def __len__(self):
        return len(self.filenames_array)

feats_name = 'mspec'
Mel_train = FloatDataSource(vox_dir + '/' + 'fnames.train', vox_dir + '/' + 'etc/falcon_feats.desc', feats_name,
                            vox_dir + '/' + 'festival/falcon_' + feats_name)
Mel_val = FloatDataSource(vox_dir + '/' + 'fnames.val', vox_dir + '/' + 'etc/falcon_feats.desc', feats_name,
                          vox_dir + '/' + 'festival/falcon_' + feats_name)
Mel_test = FloatDataSource(vox_dir + '/' + 'fnames.test', vox_dir + '/' + 'etc/falcon_feats.desc', feats_name,
                          vox_dir + '/' + 'festival/falcon_' + feats_name)

class AudioSearchDataset(object):
    def __init__(self, Mel):
        self.Mel = Mel

    def __getitem__(self, idx):
        mel = self.Mel[idx]
        # take a random audio file and use it to provide negative query (assuming that it won't be same as idx)
        query_idx = np.random.randint(len(self.Mel))
        return mel, self.Mel[query_idx]

    def __len__(self):
        return len(self.Mel)

def _pad_2d(x, max_len):
    return np.pad(x, [(0, max_len - len(x)), (0, 0)],
                  mode="constant", constant_values=0)

def collate_fn_audiosearch(batch):
    """Create batch
    args:
        batch: list of search audio mel and negative sampling audio mel
    """
    # print("collate: ",  batch[0][0].shape)
    search = [x[0][:300] for x in batch]
    search_audio_lengths = [len(x) for x in search]
    max_audio_len = np.max(search_audio_lengths) + 1
    min_audio_len = np.min(search_audio_lengths) + 1
    # print("min, max: ", min_audio_len, max_audio_len)
    query_length = 50  # keeping fixed for now
    query_start_idx = np.random.randint(max_audio_len - query_length)
    # is it good to pad and then extract query? if the difference between lengths is large, padding will be really bad,
    # also should we try edge padding instead of constant padding
    search = np.array([_pad_2d(x, max_audio_len) for x in search], dtype=np.float)
    search_batch = torch.FloatTensor(search)
    pos_query = search_batch[:, query_start_idx: query_start_idx + query_length]

    neg_audio_lengths = [len(x[1]) for x in batch]
    max_neg_audio_len = np.max(neg_audio_lengths) + 1
    neg_query_start_idx = np.random.randint(max_neg_audio_len - query_length)
    neg_query = np.array([_pad_2d(x[1], max_neg_audio_len) for x in batch], dtype=np.float)
    neg_query_batch = torch.FloatTensor(neg_query)
    neg_query = neg_query_batch[:, neg_query_start_idx: neg_query_start_idx+query_length]

    return search_batch, pos_query, neg_query

batch_size = 4
num_workers = 0

trainset = AudioSearchDataset(Mel_train)
train_loader = data_utils.DataLoader(
    trainset, batch_size=batch_size,
    num_workers=num_workers, shuffle=True,
    collate_fn=collate_fn_audiosearch, pin_memory=False)

valset = AudioSearchDataset(Mel_val)
val_loader = data_utils.DataLoader(
    valset, batch_size=batch_size,
    num_workers=num_workers, shuffle=True,
    collate_fn=collate_fn_audiosearch, pin_memory=False)

testset = AudioSearchDataset(Mel_test)
test_loader = data_utils.DataLoader(
    valset, batch_size=batch_size,
    num_workers=num_workers, shuffle=False,
    collate_fn=collate_fn_audiosearch, pin_memory=False)


cuda = torch.cuda.is_available()
device = torch.device("cuda" if cuda else "cpu")

class SequenceWise(nn.Module):
    def __init__(self, module):
        """
        Collapses input of dim T*N*H to (T*N)*H, and applies to a module.
        Allows handling of variable sequence lengths and minibatch sizes.
        :param module: Module to apply input to.
        """
        super(SequenceWise, self).__init__()
        self.module = module

    def forward(self, x):
        batch_size, time_steps = x.size(0), x.size(1)
        x = x.contiguous()
        x = x.view(batch_size * time_steps, -1)
        x = self.module(x)
        x = x.contiguous()
        x = x.view(batch_size, time_steps, -1)
        return x

    def __repr__(self):
        tmpstr = self.__class__.__name__ + ' (\n'
        tmpstr += self.module.__repr__()
        tmpstr += ')'
        return tmpstr


class AudiosearchModel(nn.Module):

    def __init__(self):
        super(AudiosearchModel, self).__init__()
        self.bert = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')
        self.mel2bert = SequenceWise(nn.Linear(80, 768))

    def forward(self, embeds, labels):
        x = torch.relu(self.mel2bert(embeds))
        loss, logits = self.bert(inputs_embeds=x, input_ids=None, token_type_ids=None, attention_mask=None, next_sentence_label=labels)
        return loss, logits
    
model = AudiosearchModel()

# model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model.to(device)
lr = 3e-5
# optimizer = optim.SGD(model.parameters(), lr=lr)
optimizer = AdamW(model.parameters(), lr=lr)
num_epochs = 1000

print("len(train_loader): ", len(train_loader), "batch_size: ", batch_size,
      "len(train_loader)//batch_size - 1: ", len(train_loader) // batch_size - 1)

def train(model, train_loader, valid_loader, test_loader, optimizer, num_epochs):
    for epoch in range(num_epochs):
        print("Epoch {}:".format(epoch))
        model.train()
        num_correct = 0
        running_loss = 0.
        num_total = 0
        label_ids = list()
        preds_list = list()
        for batch_num, d in enumerate(train_loader):
            pos, pos_sub, neg_sub = d[0], d[1], d[2]
            # print(pos.shape)
            # print(pos_sub.shape)
            # print(neg_sub.shape)
            labels = torch.cuda.LongTensor(np.zeros((pos.shape[0] * 2), float))
            labels[pos.shape[0]:] = 1
            # print(labels.shape)
            labels.to(device)
            optimizer.zero_grad()
            x = torch.cat([pos, pos_sub], dim=1)
            y = torch.cat([pos, neg_sub], dim=1)
            # print(x.shape)
            # print(y.shape)
            pairofstrings = torch.cat([x, y], dim=0)
            # pairofstrings = pairofstrings.transpose(1,2)
            # print(pairofstrings.shape)
            del pos
            del pos_sub
            del neg_sub
            # encoded_batch = tokenizer.batch_encode_plus(pairofstrings, add_special_tokens=True, return_tensors='pt',
            #                                             max_length=512, return_special_tokens_masks=True,
            #                                             pad_to_max_length=True)
            # attention_mask = (encoded_batch['attention_mask'] - encoded_batch['special_tokens_mask']).to(device)
            # input_ids, token_type_ids = pairofstrings.to(device), encoded_batch['token_type_ids'].to(
            #     device)
            input_ids = pairofstrings.to(device)
            loss, logits = model(input_ids, labels)
            # print(logits)
            predicted = torch.max(logits, 1)[1]
            # predicted = torch.max(logits, 1)[1]
            num_total += labels.size(0)
            # print("predicted:", predicted)
            # print("labels:", labels)
            num_correct += (predicted == labels).sum().item()

            # loss = criterion(outputs, labels)
            loss.backward()
            running_loss += loss.item()

            optimizer.step()
            # print(input_ids.shape)
            # print(labels.shape)
            batch_size, max_seq_len, _ = input_ids.shape
            for b in range(batch_size):
                label_ids_temp = []
                preds_list_temp = []
                label_ids_temp.append(labels[b].item())
                preds_list_temp.append(predicted[b].item())
                label_ids.extend(label_ids_temp.copy())
                preds_list.extend(preds_list_temp.copy())

            del labels
            del loss

            if batch_num % 100 == 0 or batch_num == len(train_loader) // batch_size - 1:
                print("acc : ", (num_correct) / num_total, "loss: ", running_loss, "batch_num:", batch_num)
                torch.save(model.state_dict(), 'model.npy')
                torch.save(optimizer.state_dict(), 'optimizer.npy')

        print('Train Accuracy: {}'.format(num_correct / num_total),
              'Average Train Loss: {}'.format(running_loss / len(train_loader)))
        # print(type(label_ids), type(preds_list))
        # # label_ids, preds_list = label_ids.flatten(), preds_list.flatten()
        # print(label_ids, preds_list)
        # print(len(label_ids), len(preds_list))
        print("Precision: " + str(precision_score(label_ids, preds_list)))
        print("Recall: " + str(recall_score(label_ids, preds_list)))
        print("F1: " + str(f1_score(label_ids, preds_list)))

        if epoch % 1 == 0:
            ep_num = epoch + 1
            torch.save(model.state_dict(), 'model' + str(ep_num) + '.npy')
            torch.save(optimizer.state_dict(), 'optimizer' + str(ep_num) + '.npy')

        model.eval()
        num_correct = 0
        running_loss = 0.
        num_total = 0
        with torch.no_grad():
            label_ids = list()
            preds_list = list()
            for batch_num, d in enumerate(valid_loader):
                pos, pos_sub, neg_sub = d[0], d[1], d[2]
                labels = torch.cuda.LongTensor(np.zeros((pos.shape[0] * 2), float))
                labels[pos.shape[0]:] = 1
                # labels = torch.cuda.LongTensor(np.zeros((len(pos) * 2), float))
                # labels[len(pos):] = 1
                labels.to(device)
                # pairofstrings = list(zip(pos, pos_sub))
                # pairofstrings.extend(list(zip(pos, neg_sub)))

                x = torch.cat([pos, pos_sub], dim=1)
                y = torch.cat([pos, neg_sub], dim=1)
                # print(x.shape)
                # print(y.shape)
                pairofstrings = torch.cat([x, y], dim=0)
                # pairofstrings = pairofstrings.transpose(1,2)
                # print(pairofstrings.shape)
                del pos
                del pos_sub
                del neg_sub
                # encoded_batch = tokenizer.batch_encode_plus(pairofstrings, add_special_tokens=True, return_tensors='pt',
                #                                             max_length=512, return_special_tokens_masks=True,
                #                                             pad_to_max_length=True)
                # attention_mask = (encoded_batch['attention_mask'] - encoded_batch['special_tokens_mask']).to(device)
                # input_ids, token_type_ids = pairofstrings.to(device), encoded_batch['token_type_ids'].to(
                #     device)
                input_ids = pairofstrings.to(device)
                loss, logits = model(input_ids, labels)
                predicted = torch.max(logits, 1)[1]
                # print("labels:")
                num_total += labels.size(0)
                num_correct += (predicted == labels).sum().item()
                running_loss += loss.item()
                print("predicted, labels:", predicted.cpu().detach().numpy(), labels.cpu().detach().numpy())
                batch_size, max_seq_len, _ = input_ids.shape
                for b in range(batch_size):
                    label_ids_temp = []
                    preds_list_temp = []
                    label_ids_temp.append(labels[b].item())
                    preds_list_temp.append(predicted[b].item())
                    label_ids.extend(label_ids_temp.copy())
                    preds_list.extend(preds_list_temp.copy())

                del labels
                del loss

        print('Validation Accuracy: {}'.format(num_correct / num_total),
              'Average Validation Loss: {}'.format(running_loss / len(valid_loader)))
        print("Precision: " + str(precision_score(label_ids, preds_list)))
        print("Recall: " + str(recall_score(label_ids, preds_list)))
        print("F1: " + str(f1_score(label_ids, preds_list)))
        num_correct = 0
        running_loss = 0.
        num_total = 0
        with torch.no_grad():
            label_ids = list()
            preds_list = list()
            for batch_num, d in enumerate(test_loader):
                pos, pos_sub, neg_sub = d[0], d[1], d[2]
                labels = torch.cuda.LongTensor(np.zeros((pos.shape[0] * 2), float))
                labels[pos.shape[0]:] = 1
                # labels = torch.cuda.LongTensor(np.zeros((len(pos) * 2), float))
                # labels[len(pos):] = 1
                labels.to(device)
                x = torch.cat([pos, pos_sub], dim=1)
                y = torch.cat([pos, neg_sub], dim=1)
                # print(x.shape)
                # print(y.shape)
                pairofstrings = torch.cat([x, y], dim=0)
                # pairofstrings = pairofstrings.transpose(1,2)
                # print(pairofstrings.shape)
                del pos
                del pos_sub
                del neg_sub
                # encoded_batch = tokenizer.batch_encode_plus(pairofstrings, add_special_tokens=True, return_tensors='pt',
                #                                             max_length=512, return_special_tokens_masks=True,
                #                                             pad_to_max_length=True)
                # attention_mask = (encoded_batch['attention_mask'] - encoded_batch['special_tokens_mask']).to(device)
                # input_ids, token_type_ids = pairofstrings.to(device), encoded_batch['token_type_ids'].to(
                #     device)
                input_ids = pairofstrings.to(device)
                loss, logits = model(input_ids, labels)
                # loss, logits = model(input_ids, token_type_ids=None, attention_mask=None,
                #                      next_sentence_label=labels)
                predicted = torch.max(logits, 1)[1]
                # print("labels:")
                num_total += labels.size(0)
                num_correct += (predicted == labels).sum().item()
                running_loss += loss.item()
                batch_size, max_seq_len, _ = input_ids.shape
                for b in range(batch_size):
                    label_ids_temp = []
                    preds_list_temp = []
                    label_ids_temp.append(labels[b].item())
                    preds_list_temp.append(predicted[b].item())
                    label_ids.extend(label_ids_temp.copy())
                    preds_list.extend(preds_list_temp.copy())

                del labels
                del loss
        print('Test Accuracy: {}'.format(num_correct / num_total),
              'Average Test Loss: {}'.format(running_loss / len(valid_loader)))
        print("Precision: " + str(precision_score(label_ids, preds_list)))
        print("Recall: " + str(recall_score(label_ids, preds_list)))
        print("F1: " + str(f1_score(label_ids, preds_list)))

print(device)

train(model, train_loader, val_loader, test_loader, optimizer, num_epochs)

torch.save(model.state_dict(), 'model_final.npy')
torch.save(optimizer.state_dict(), 'optimizer_final.npy')
#
# from google.colab import drive
# drive.mount('/content/drive')

# !nvidia-smi
