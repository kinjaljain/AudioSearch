{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_NSP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC2Ue2SRZrPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "!pip install seqeval"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwNK_oEYdx8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertForNextSentencePrediction, BertTokenizer, AdamW\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "import string\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ua0Zk67axto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "        self.data = None\n",
        "        with open(path, 'r') as f:\n",
        "            self.data = f.readlines()\n",
        "            self.data = [x.translate(str.maketrans('', '', string.punctuation)) for x in self.data]\n",
        "            self.data = [x for x in self.data if len(x.split())>=2]\n",
        "            self.lengths = [len(y.split()) for y in self.data]\n",
        "            # print(self.lengths)\n",
        "            self.substr_len = 2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.lengths[index]\n",
        "        try:\n",
        "            neg_x = self.data[index + 1]\n",
        "            neg_y = self.lengths[index + 1]\n",
        "        except:\n",
        "            neg_x = self.data[index - 1]\n",
        "            neg_y = self.lengths[index - 1]\n",
        "        try:\n",
        "          rand_idx_pos = np.random.randint(0, y - self.substr_len - 1)\n",
        "        except:\n",
        "          rand_idx_pos = np.random.randint(0, y - 1)\n",
        "        try:\n",
        "          rand_idx_neg = np.random.randint(0, neg_y - self.substr_len - 1)\n",
        "        except:\n",
        "          rand_idx_neg = np.random.randint(0, neg_y - 1)\n",
        "        \n",
        "\n",
        "\n",
        "        return x, \\\n",
        "               \" \".join(x.split()[rand_idx_pos:rand_idx_pos + self.substr_len]), \\\n",
        "               \" \".join(neg_x.split()[rand_idx_neg:rand_idx_neg + self.substr_len])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DDVWVrra3li",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/DirectedStudy/BERT_NSP/wikitext-2'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvvk9r5od5iq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = CustomDataset(os.path.join(path, \"train_.txt\"))\n",
        "valid_dataset = CustomDataset(os.path.join(path, \"valid_.txt\"))\n",
        "test_dataset = CustomDataset(os.path.join(path, \"test_.txt\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9X6cP_Md8Kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBl7AyPBeFT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 4\n",
        "num_workers = 8 if cuda else multiprocessing.cpu_count()\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
        "valid_dataloader = DataLoader(valid_dataset, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size, num_workers=num_workers)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ0F_aDNeILN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model.to(device)\n",
        "lr = 3e-5\n",
        "# optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "num_epochs = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14gxTK7feJyE",
        "colab_type": "code",
        "outputId": "06952aa4-6296-4eaf-aef5-90fb1a7cf415",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"len(trainloader): \", len(train_dataloader), \"batch_size: \", batch_size,\n",
        "      \"len(train_dataloader)//batch_size - 1: \", len(train_dataloader) // batch_size - 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(trainloader):  14952 batch_size:  4 len(train_dataloader)//batch_size - 1:  3737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDAUP3jYeL6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader, valid_loader, test_loader, optimizer, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"Epoch {}:\".format(epoch))\n",
        "        model.train()\n",
        "        num_correct = 0\n",
        "        running_loss = 0.\n",
        "        num_total = 0\n",
        "        label_ids = list()\n",
        "        preds_list = list()\n",
        "        for batch_num, d in enumerate(train_loader):\n",
        "            pos, pos_sub, neg_sub = d[0], d[1], d[2]\n",
        "            # print(pos)\n",
        "            # print(pos_sub)\n",
        "            # print(neg_sub)\n",
        "            labels = torch.cuda.LongTensor(np.zeros((len(pos) * 2), float))\n",
        "            labels[len(pos):] = 1\n",
        "            # print(labels)\n",
        "            labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pairofstrings = list(zip(pos, pos_sub))\n",
        "            pairofstrings.extend(list(zip(pos, neg_sub)))\n",
        "            del pos\n",
        "            del pos_sub\n",
        "            del neg_sub\n",
        "            encoded_batch = tokenizer.batch_encode_plus(pairofstrings, add_special_tokens=True, return_tensors='pt',\n",
        "                                                        return_special_tokens_masks=True, max_length=512,\n",
        "                                                        pad_to_max_length=True)\n",
        "            attention_mask = (encoded_batch['attention_mask'] - encoded_batch['special_tokens_mask']).to(device)\n",
        "            input_ids, token_type_ids = encoded_batch['input_ids'].to(device), encoded_batch['token_type_ids'].to(\n",
        "                device)\n",
        "            loss, logits = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,\n",
        "                                 next_sentence_label=labels)\n",
        "            # print(logits)\n",
        "            predicted = torch.max(logits, 1)[1]\n",
        "            # predicted = torch.max(logits, 1)[1]\n",
        "            num_total += labels.size(0)\n",
        "            # print(\"predicted:\", predicted)\n",
        "            # print(\"labels:\", labels)\n",
        "            num_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            optimizer.step()\n",
        "            batch_size, max_seq_len = input_ids.shape\n",
        "            for b in range(batch_size):\n",
        "                label_ids_temp = []\n",
        "                preds_list_temp = []\n",
        "                for l in range(max_seq_len):\n",
        "                    label_ids_temp.append(labels[b].item())\n",
        "                    preds_list_temp.append(predicted[b].item())\n",
        "                label_ids.append(label_ids_temp.copy())\n",
        "                preds_list.append(preds_list_temp.copy())\n",
        "\n",
        "            del labels\n",
        "            del loss\n",
        "\n",
        "            if batch_num % 100 == 0 or batch_num == len(train_dataloader) // batch_size - 1:\n",
        "                print(\"acc : \", (num_correct) / num_total, \"batch_num:\", batch_num)\n",
        "                torch.save(model.state_dict(), 'model.npy')\n",
        "                torch.save(optimizer.state_dict(), 'optimizer.npy')\n",
        "\n",
        "        print('Train Accuracy: {}'.format(num_correct / num_total),\n",
        "              'Average Train Loss: {}'.format(running_loss / len(train_loader)))\n",
        "        print(\"Precision: \" + str(precision_score(label_ids, preds_list)))\n",
        "        print(\"Recall: \" + str(recall_score(label_ids, preds_list)))\n",
        "        print(\"F1: \" + str(f1_score(label_ids, preds_list)))\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            ep_num = epoch + 1\n",
        "            torch.save(model.state_dict(), 'model' + str(ep_num) + '.npy')\n",
        "            torch.save(optimizer.state_dict(), 'optimizer' + str(ep_num) + '.npy')\n",
        "\n",
        "        model.eval()\n",
        "        num_correct = 0\n",
        "        running_loss = 0.\n",
        "        num_total = 0\n",
        "        with torch.no_grad():\n",
        "            label_ids = list()\n",
        "            preds_list = list()\n",
        "            for batch_num, d in enumerate(valid_loader):\n",
        "                pos, pos_sub, neg_sub = d[0], d[1], d[2]\n",
        "                labels = torch.cuda.LongTensor(np.zeros((len(pos) * 2), float))\n",
        "                labels[len(pos):] = 1\n",
        "                labels.to(device)\n",
        "                pairofstrings = list(zip(pos, pos_sub))\n",
        "                pairofstrings.extend(list(zip(pos, neg_sub)))\n",
        "                del pos\n",
        "                del pos_sub\n",
        "                del neg_sub\n",
        "                encoded_batch = tokenizer.batch_encode_plus(pairofstrings, add_special_tokens=True, return_tensors='pt',\n",
        "                                                            max_length=512, return_special_tokens_masks=True,\n",
        "                                                            pad_to_max_length=True)\n",
        "                attention_mask = (encoded_batch['attention_mask'] - encoded_batch['special_tokens_mask']).to(device)\n",
        "                input_ids, token_type_ids = encoded_batch['input_ids'].to(device), encoded_batch['token_type_ids'].to(\n",
        "                    device)\n",
        "                loss, logits = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,\n",
        "                                     next_sentence_label=labels)\n",
        "                predicted = torch.max(logits, 1)[1]\n",
        "                # print(\"labels:\")\n",
        "                num_total += labels.size(0)\n",
        "                num_correct += (predicted == labels).sum().item()\n",
        "                running_loss += loss.item()\n",
        "                print(\"predicted, labels:\", predicted.cpu().detach().numpy(), labels.cpu().detach().numpy())\n",
        "                batch_size, max_seq_len = input_ids.shape\n",
        "                for b in range(batch_size):\n",
        "                    label_ids_temp = []\n",
        "                    preds_list_temp = []\n",
        "                    for l in range(max_seq_len):\n",
        "                        label_ids_temp.append(labels[b].item())\n",
        "                        preds_list_temp.append(predicted[b].item())\n",
        "                    label_ids.append(label_ids_temp.copy())\n",
        "                    preds_list.append(preds_list_temp.copy())\n",
        "\n",
        "                del labels\n",
        "                del loss\n",
        "\n",
        "        print('Validation Accuracy: {}'.format(num_correct / num_total),\n",
        "              'Average Validation Loss: {}'.format(running_loss / len(valid_loader)))\n",
        "        print(\"Precision: \" + str(precision_score(label_ids, preds_list)))\n",
        "        print(\"Recall: \" + str(recall_score(label_ids, preds_list)))\n",
        "        print(\"F1: \" + str(f1_score(label_ids, preds_list)))\n",
        "        num_correct = 0\n",
        "        running_loss = 0.\n",
        "        num_total = 0\n",
        "        with torch.no_grad():\n",
        "            label_ids = list()\n",
        "            preds_list = list()\n",
        "            for batch_num, d in enumerate(test_loader):\n",
        "                pos, pos_sub, neg_sub = d[0], d[1], d[2]\n",
        "                labels = torch.cuda.LongTensor(np.zeros((len(pos) * 2), float))\n",
        "                labels[len(pos):] = 1\n",
        "                labels.to(device)\n",
        "                pairofstrings = list(zip(pos, pos_sub))\n",
        "                pairofstrings.extend(list(zip(pos, neg_sub)))\n",
        "                del pos\n",
        "                del pos_sub\n",
        "                del neg_sub\n",
        "                encoded_batch = tokenizer.batch_encode_plus(pairofstrings, add_special_tokens=True, return_tensors='pt',\n",
        "                                                            max_length=512, return_special_tokens_masks=True,\n",
        "                                                            pad_to_max_length=True)\n",
        "                attention_mask = (encoded_batch['attention_mask'] - encoded_batch['special_tokens_mask']).to(device)\n",
        "                input_ids, token_type_ids = encoded_batch['input_ids'].to(device), encoded_batch['token_type_ids'].to(\n",
        "                    device)\n",
        "                loss, logits = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,\n",
        "                                     next_sentence_label=labels)\n",
        "                predicted = torch.max(logits, 1)[1]\n",
        "                # print(\"labels:\")\n",
        "                num_total += labels.size(0)\n",
        "                num_correct += (predicted == labels).sum().item()\n",
        "                running_loss += loss.item()\n",
        "                batch_size, max_seq_len = input_ids.shape\n",
        "                for b in range(batch_size):\n",
        "                    label_ids_temp = []\n",
        "                    preds_list_temp = []\n",
        "                    for l in range(max_seq_len):\n",
        "                        label_ids_temp.append(labels[b].item())\n",
        "                        preds_list_temp.append(predicted[b].item())\n",
        "                    label_ids.append(label_ids_temp.copy())\n",
        "                    preds_list.append(preds_list_temp.copy())\n",
        "\n",
        "                del labels\n",
        "                del loss\n",
        "        print('Test Accuracy: {}'.format(num_correct / num_total),\n",
        "              'Average Test Loss: {}'.format(running_loss / len(valid_loader)))\n",
        "        print(\"Precision: \" + str(precision_score(label_ids, preds_list)))\n",
        "        print(\"Recall: \" + str(recall_score(label_ids, preds_list)))\n",
        "        print(\"F1: \" + str(f1_score(label_ids, preds_list)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40m_tSQxipyw",
        "colab_type": "code",
        "outputId": "e5f16103-74dc-4ba1-bfd8-a570054b3244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY-rFpPReO9I",
        "colab_type": "code",
        "outputId": "7ae5d308-1b1b-4e3f-d73d-de0cdaf322ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train(model, train_dataloader, valid_dataloader, test_dataloader, optimizer, num_epochs)\n",
        "\n",
        "torch.save(model.state_dict(), 'model_final.npy')\n",
        "torch.save(optimizer.state_dict(), 'optimizer_final.npy')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0:\n",
            "acc :  0.75 batch_num: 0\n",
            "acc :  0.7561881188118812 batch_num: 100\n",
            "acc :  0.8314676616915423 batch_num: 200\n",
            "acc :  0.8538205980066446 batch_num: 300\n",
            "acc :  0.8728179551122195 batch_num: 400\n",
            "acc :  0.8827345309381237 batch_num: 500\n",
            "acc :  0.8930948419301165 batch_num: 600\n",
            "acc :  0.8987161198288159 batch_num: 700\n",
            "acc :  0.9026217228464419 batch_num: 800\n",
            "acc :  0.9049667036625971 batch_num: 900\n",
            "acc :  0.9104645354645354 batch_num: 1000\n",
            "acc :  0.9132606721162579 batch_num: 1100\n",
            "acc :  0.9170482930890924 batch_num: 1200\n",
            "acc :  0.9190046118370484 batch_num: 1300\n",
            "acc :  0.9212169878658102 batch_num: 1400\n",
            "acc :  0.9234676882078614 batch_num: 1500\n",
            "acc :  0.9248126171143035 batch_num: 1600\n",
            "acc :  0.9273956496178718 batch_num: 1700\n",
            "acc :  0.9284425319267073 batch_num: 1800\n",
            "acc :  0.9293739967897271 batch_num: 1868\n",
            "acc :  0.9302340873224618 batch_num: 1900\n",
            "acc :  0.9320964517741129 batch_num: 2000\n",
            "acc :  0.9333650642551166 batch_num: 2100\n",
            "acc :  0.934234438891413 batch_num: 2200\n",
            "acc :  0.9355714906562365 batch_num: 2300\n",
            "acc :  0.9363806747188671 batch_num: 2400\n",
            "acc :  0.9371751299480208 batch_num: 2500\n",
            "acc :  0.9374279123414071 batch_num: 2600\n",
            "acc :  0.9380784894483525 batch_num: 2700\n",
            "acc :  0.9392181363798643 batch_num: 2800\n",
            "acc :  0.9396759738021372 batch_num: 2900\n",
            "acc :  0.9391869376874376 batch_num: 3000\n",
            "acc :  0.9396162528216704 batch_num: 3100\n",
            "acc :  0.9395110902842861 batch_num: 3200\n",
            "acc :  0.9399045743714026 batch_num: 3300\n",
            "acc :  0.9404219347250808 batch_num: 3400\n",
            "acc :  0.9410882604970009 batch_num: 3500\n",
            "acc :  0.9413357400722022 batch_num: 3600\n",
            "acc :  0.9418062685760605 batch_num: 3700\n",
            "acc :  0.9422520389371218 batch_num: 3800\n",
            "acc :  0.942867213534991 batch_num: 3900\n",
            "acc :  0.9430142464383904 batch_num: 4000\n",
            "acc :  0.9437941965374299 batch_num: 4100\n",
            "acc :  0.9442394667936206 batch_num: 4200\n",
            "acc :  0.9444896535689374 batch_num: 4300\n",
            "acc :  0.9448420813451488 batch_num: 4400\n",
            "acc :  0.9451788491446346 batch_num: 4500\n",
            "acc :  0.9454194740273854 batch_num: 4600\n",
            "acc :  0.9456498617315465 batch_num: 4700\n",
            "acc :  0.9460529056446574 batch_num: 4800\n",
            "acc :  0.9462609671495613 batch_num: 4900\n",
            "acc :  0.9463857228554289 batch_num: 5000\n",
            "acc :  0.9466526171338954 batch_num: 5100\n",
            "acc :  0.9471976542972506 batch_num: 5200\n",
            "acc :  0.9475099037917374 batch_num: 5300\n",
            "acc :  0.9478105906313645 batch_num: 5400\n",
            "acc :  0.9481685148154881 batch_num: 5500\n",
            "acc :  0.9484243885020532 batch_num: 5600\n",
            "acc :  0.9487370636730398 batch_num: 5700\n",
            "acc :  0.9489527667643509 batch_num: 5800\n",
            "acc :  0.9491399762752076 batch_num: 5900\n",
            "acc :  0.9494875854024329 batch_num: 6000\n",
            "acc :  0.9496803802655303 batch_num: 6100\n",
            "acc :  0.9498266408643767 batch_num: 6200\n",
            "acc :  0.9501666402158387 batch_num: 6300\n",
            "acc :  0.9505155444461802 batch_num: 6400\n",
            "acc :  0.9507383479464697 batch_num: 6500\n",
            "acc :  0.9510680199969701 batch_num: 6600\n",
            "acc :  0.9512945828980749 batch_num: 6700\n",
            "acc :  0.9514961035141891 batch_num: 6800\n",
            "acc :  0.9516374438487176 batch_num: 6900\n",
            "acc :  0.9517747464647908 batch_num: 7000\n",
            "acc :  0.9519961977186312 batch_num: 7100\n",
            "acc :  0.9522809332037218 batch_num: 7200\n",
            "acc :  0.9525921106697712 batch_num: 7300\n",
            "acc :  0.9527090933657614 batch_num: 7400\n",
            "acc :  0.9529729369417411 batch_num: 7500\n",
            "acc :  0.9532627285883436 batch_num: 7600\n",
            "acc :  0.9534151408907934 batch_num: 7700\n",
            "acc :  0.9534835277528522 batch_num: 7800\n",
            "acc :  0.9535343627388938 batch_num: 7900\n",
            "acc :  0.9538338957630296 batch_num: 8000\n",
            "acc :  0.9537865695593136 batch_num: 8100\n",
            "acc :  0.9539537861236435 batch_num: 8200\n",
            "acc :  0.954162149138658 batch_num: 8300\n",
            "acc :  0.9543357933579336 batch_num: 8400\n",
            "acc :  0.9545053523114928 batch_num: 8500\n",
            "acc :  0.9547000348796651 batch_num: 8600\n",
            "acc :  0.9547896793472015 batch_num: 8700\n",
            "acc :  0.9549483013293943 batch_num: 8800\n",
            "acc :  0.9549910122458151 batch_num: 8900\n",
            "acc :  0.9551438729030107 batch_num: 9000\n",
            "acc :  0.9551560268102406 batch_num: 9100\n",
            "acc :  0.9553309422888816 batch_num: 9200\n",
            "acc :  0.9555558542092248 batch_num: 9300\n",
            "acc :  0.9556031273268801 batch_num: 9400\n",
            "acc :  0.9556888748552784 batch_num: 9500\n",
            "acc :  0.9557207582543485 batch_num: 9600\n",
            "acc :  0.9558292959488712 batch_num: 9700\n",
            "acc :  0.9559483726150393 batch_num: 9800\n",
            "acc :  0.9560650439349561 batch_num: 9900\n",
            "acc :  0.9561168883111689 batch_num: 10000\n",
            "acc :  0.9561429561429562 batch_num: 10100\n",
            "acc :  0.9562175276933634 batch_num: 10200\n",
            "acc :  0.9564362683234637 batch_num: 10300\n",
            "acc :  0.9565666762811268 batch_num: 10400\n",
            "acc :  0.9566231787448815 batch_num: 10500\n",
            "acc :  0.956784737288935 batch_num: 10600\n",
            "acc :  0.9568615082702551 batch_num: 10700\n",
            "acc :  0.9570757337283585 batch_num: 10800\n",
            "acc :  0.9572286946151729 batch_num: 10900\n",
            "acc :  0.9573334242341606 batch_num: 11000\n",
            "acc :  0.9574925682370957 batch_num: 11100\n",
            "acc :  0.957637710918668 batch_num: 11200\n",
            "acc :  0.957791345898593 batch_num: 11300\n",
            "acc :  0.957931321813876 batch_num: 11400\n",
            "acc :  0.9580688635770802 batch_num: 11500\n",
            "acc :  0.9582148090681838 batch_num: 11600\n",
            "acc :  0.9583689428253995 batch_num: 11700\n",
            "acc :  0.9584357257859504 batch_num: 11800\n",
            "acc :  0.9586169229476514 batch_num: 11900\n",
            "acc :  0.9588471794017165 batch_num: 12000\n",
            "acc :  0.9589083546814313 batch_num: 12100\n",
            "acc :  0.9590299975411851 batch_num: 12200\n",
            "acc :  0.9591191773026583 batch_num: 12300\n",
            "acc :  0.9592875574550439 batch_num: 12400\n",
            "acc :  0.9593432525397968 batch_num: 12500\n",
            "acc :  0.9593881437981112 batch_num: 12600\n",
            "acc :  0.9594520116526257 batch_num: 12700\n",
            "acc :  0.9595148816498711 batch_num: 12800\n",
            "acc :  0.9595477094798853 batch_num: 12900\n",
            "acc :  0.9597530959156988 batch_num: 13000\n",
            "acc :  0.9598790168689413 batch_num: 13100\n",
            "acc :  0.9599935610938565 batch_num: 13200\n",
            "acc :  0.9601909630854822 batch_num: 13300\n",
            "acc :  0.9602828147153197 batch_num: 13400\n",
            "acc :  0.9603918228279387 batch_num: 13500\n",
            "acc :  0.9604716564958459 batch_num: 13600\n",
            "acc :  0.9606233121669951 batch_num: 13700\n",
            "acc :  0.9606278530541265 batch_num: 13800\n",
            "acc :  0.9607582188331775 batch_num: 13900\n",
            "acc :  0.9609224341118492 batch_num: 14000\n",
            "acc :  0.9608981632508333 batch_num: 14100\n",
            "acc :  0.960953453982114 batch_num: 14200\n",
            "acc :  0.9611128592406125 batch_num: 14300\n",
            "acc :  0.9611919311158947 batch_num: 14400\n",
            "acc :  0.9613043928004965 batch_num: 14500\n",
            "acc :  0.961398191904664 batch_num: 14600\n",
            "acc :  0.9612951499897966 batch_num: 14700\n",
            "acc :  0.9612272819404094 batch_num: 14800\n",
            "acc :  0.9613029326890813 batch_num: 14900\n",
            "Train Accuracy: 0.9613577460078588 Average Train Loss: 0.12097865187674123\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-47f0e2d37473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_final.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer_final.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-783035d211a0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, valid_loader, test_loader, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     64\u001b[0m         print('Train Accuracy: {}'.format(num_correct / num_total),\n\u001b[1;32m     65\u001b[0m               'Average Train Loss: {}'.format(running_loss / len(train_loader)))\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recall: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, average, suffix)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;36m0.50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \"\"\"\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mtrue_entities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0mpred_entities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py\u001b[0m in \u001b[0;36mget_entities\u001b[0;34m(seq, suffix)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mtype_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mtype_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_RqKxiuc5oT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFq5VSG2lDej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}